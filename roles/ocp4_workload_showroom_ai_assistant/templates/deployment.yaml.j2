---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ ocp4_workload_showroom_ai_assistant_name }}
  namespace: {{ _showroom_namespace }}
  labels:
    app.kubernetes.io/name: {{ ocp4_workload_showroom_ai_assistant_name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ ocp4_workload_showroom_ai_assistant_name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ ocp4_workload_showroom_ai_assistant_name }}
    spec:
      serviceAccountName: {{ ocp4_workload_showroom_ai_assistant_name }}
      initContainers:
      # Pre-load vector embeddings from pre-built image
      - name: vector-loader
        image: {{ ocp4_workload_showroom_ai_assistant_rag_image }}
        imagePullPolicy: Always
        env:
        - name: SOURCE_DIR
          value: "/vector-data"
        - name: TARGET_DIR
          value: "/.llama"
        volumeMounts:
        - name: llamastack-data
          mountPath: /.llama
        command: ["/bin/bash", "-c", "/copy-vectors.sh"]
        resources:
          requests:
            memory: "{{ ocp4_workload_showroom_ai_assistant_vector_loader_requests_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_vector_loader_requests_cpu }}"
          limits:
            memory: "{{ ocp4_workload_showroom_ai_assistant_vector_loader_limits_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_vector_loader_limits_cpu }}"
      containers:
      # Backend API Container
      - name: backend
        image: {{ ocp4_workload_showroom_ai_assistant_image }}
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: PORT
          value: "8080"
        - name: ASSISTANT_CONFIG_PATH
          value: "/app/config/assistant-config.yaml"
        - name: CONTENT_DIR
          value: "/app/rag-content"
        - name: PDF_DIR
          value: "/app/content/modules/ROOT/assets/techdocs"
        - name: LLAMA_STACK_URL
          value: "http://localhost:8321"
        envFrom:
        - secretRef:
            name: {{ ocp4_workload_showroom_ai_assistant_name }}-secrets
        volumeMounts:
        - name: config
          mountPath: /app/config/assistant-config.yaml
          subPath: assistant-config.yaml
        livenessProbe:
          httpGet:
            path: /api/health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /api/health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
        resources:
          requests:
            memory: "{{ ocp4_workload_showroom_ai_assistant_backend_requests_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_backend_requests_cpu }}"
          limits:
            memory: "{{ ocp4_workload_showroom_ai_assistant_backend_limits_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_backend_limits_cpu }}"

      # MCP Kubernetes Server Container
      - name: mcp-server
        image: {{ ocp4_workload_showroom_ai_assistant_mcp_kube_image }}
        command:
        - sh
        - -c
        - npx -y {{ ocp4_workload_showroom_ai_assistant_mcp_kube_npm_package }} --port 3000
        ports:
        - containerPort: 3000
          name: mcp
          protocol: TCP
        resources:
          requests:
            memory: "{{ ocp4_workload_showroom_ai_assistant_mcp_server_requests_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_mcp_server_requests_cpu }}"
          limits:
            memory: "{{ ocp4_workload_showroom_ai_assistant_mcp_server_limits_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_mcp_server_limits_cpu }}"
        env:
          - name: HOME
            value: /tmp
          - name: NPM_CONFIG_CACHE
            value: /tmp/.npm

      # LlamaStack Container
      - name: llamastack
        image: {{ ocp4_workload_showroom_ai_assistant_lls_image }}
        ports:
        - containerPort: 8321
          name: llama
          protocol: TCP
        envFrom:
        - secretRef:
            name: {{ ocp4_workload_showroom_ai_assistant_name }}-secrets
        volumeMounts:
        - name: llamastack-data
          mountPath: /.llama
        - name: hfcache
          mountPath: /.cache
        resources:
          requests:
            memory: "{{ ocp4_workload_showroom_ai_assistant_llamastack_requests_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_llamastack_requests_cpu }}"
          limits:
            memory: "{{ ocp4_workload_showroom_ai_assistant_llamastack_limits_memory }}"
            cpu: "{{ ocp4_workload_showroom_ai_assistant_llamastack_limits_cpu }}"

      volumes:
      - name: llamastack-data
        persistentVolumeClaim:
          claimName: {{ ocp4_workload_showroom_ai_assistant_name }}-llamastack
      - name: hfcache
        persistentVolumeClaim:
          claimName: {{ ocp4_workload_showroom_ai_assistant_name }}-hfcache
      - name: config
        configMap:
          name: {{ ocp4_workload_showroom_ai_assistant_name }}-config
